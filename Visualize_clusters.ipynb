{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import SolarDataset, normalize_standard, preprocess_clip_wrapper, preprocess_dino,load_file_names_and_classes_for_test, prepare_dataloaders, find_all_fits_files, load_filenames, load_filenames_inverse\n",
    "from utils import test_visualize_images_all_cluster_zero, visualize_batch_images_from_cluster, visualize_histograms_from_cluster, visualize_np_images_from_cluster\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from general import DATA_PATH, TEST_PATH\n",
    "from utils import *\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from sklearn.utils import check_random_state \n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "\n",
    "# Set the environment variable for the current session\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "random_seed = 45\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "random_state = check_random_state(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"outputs\"):\n",
    "    os.mkdir(\"outputs\")\n",
    "if not os.path.exists(\"outputs/isolation_forest\"):\n",
    "    os.mkdir(\"outputs/isolation_forest\")\n",
    "if not os.path.exists(\"outputs/weights\"):\n",
    "    os.mkdir(\"outputs/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 32\n",
    "# By default we will visualize files from the folder which is listed in saved saved_filtered_filenames_final.txt\n",
    "inverse=False\n",
    "mode=\"clip\"  #other option: dino, msn, mae, standard\n",
    "\n",
    "initial_steps = [\n",
    "    #{'type': 'pca', 'params': {'n_components': 0.95}},\n",
    "    {'type': 'tsne', 'params': {'n_components': 2}},\n",
    "    {'type': 'histogram_kmeans', 'params': {'n_clusters': 20}},\n",
    "    # Add other steps as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_clusters_by_median(cluster_medians, cluster_labels_unique):\n",
    "    ordered_indices = [3]\n",
    "    remaining_indices = set(cluster_labels_unqiue) - set(ordered_indices)\n",
    "    \n",
    "    while remaining_indices:\n",
    "        last_index = ordered_indices[-1]\n",
    "        # Find the closest cluster by median, excluding already chosen ones\n",
    "        closest_index = min(remaining_indices, key=lambda x: abs(cluster_medians[x] - cluster_medians[last_index]))\n",
    "        ordered_indices.append(closest_index)\n",
    "        remaining_indices.remove(closest_index)\n",
    "    \n",
    "    return ordered_indices\n",
    "\n",
    "def reorder_images_by_cluster_order(images, labels, ordered_cluster_indices):\n",
    "    reordered_images = np.concatenate([images[labels == i] for i in ordered_cluster_indices])\n",
    "    reordered_labels = np.concatenate([labels[labels == i] for i in ordered_cluster_indices])\n",
    "    return reordered_images, reordered_labels\n",
    "\n",
    "def reorder_properties(train_properties, ordered_indices):\n",
    "    reordered_properties = {}\n",
    "    for key, value in train_properties.items():\n",
    "        # Ensure the property is an array-like structure and has the same first dimension.\n",
    "        if isinstance(value, np.ndarray) : #and value.shape[0] == len(ordered_indices):\n",
    "            reordered_properties[key] = value[ordered_indices]\n",
    "        else:\n",
    "            reordered_properties[key] = value  # Keep unchanged if the property does not match the criteria.\n",
    "    return reordered_properties\n",
    "\n",
    "def filter_images_by_cluster(reordered_properties, reordered_indices, sorted_labels, indices_reduce, n=8):\n",
    "    # Initialize a dictionary to track the count of images for each cluster in indices_reduce\n",
    "    cluster_image_count = {cluster: 0 for cluster in np.unique(sorted_labels)}\n",
    "\n",
    "    # List to hold the selected indices following the original order\n",
    "    selected_reordered_indices = []\n",
    "\n",
    "    indexes_normal = np.array(range(np.max(reordered_indices)))\n",
    "\n",
    "    # Iterate over sorted_labels and reordered_indices together\n",
    "    for idx, cluster_label in zip(indexes_normal, sorted_labels):\n",
    "        if cluster_label in indices_reduce:\n",
    "            if cluster_image_count[cluster_label] < n:\n",
    "                # Add index if the cluster's image count is below the limit\n",
    "                selected_reordered_indices.append(idx)\n",
    "                cluster_image_count[cluster_label] += 1\n",
    "        else:\n",
    "            # Always include indices for clusters not in indices_reduce\n",
    "            selected_reordered_indices.append(idx)\n",
    "            cluster_image_count[cluster_label] += 1\n",
    "\n",
    "\n",
    "    # Filter each property in the reordered_properties dictionary using selected indices\n",
    "    filtered_properties = {}\n",
    "    for key, value in reordered_properties.items():\n",
    "        if isinstance(value, np.ndarray) and value.shape[0] == len(reordered_indices):\n",
    "            filtered_properties[key] = value[selected_reordered_indices]\n",
    "        else:\n",
    "            filtered_properties[key] = value\n",
    "\n",
    "    # The new_reordered_original_indices list is simply the selected_reordered_indices\n",
    "    new_reordered_original_indices = selected_reordered_indices\n",
    "\n",
    "    # Generate new sorted_labels based on the filtered selection\n",
    "    new_sorted_labels = [sorted_labels[reordered_indices.index(i)] for i in selected_reordered_indices]\n",
    "\n",
    "    return filtered_properties, new_reordered_original_indices, new_sorted_labels\n",
    "\n",
    "\n",
    "def visualize_grid_with_clusters(images, cluster_ids, grid_size=(90, 90), figsize=(18, 18), border_size=10, other_props=None):\n",
    "    colormap = cm.get_cmap('tab20b', max(cluster_ids) + 1) \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.set_xlim(0, grid_size[1] * images.shape[2]+2*border_size)\n",
    "    ax.set_ylim(0, grid_size[0] * images.shape[1]+2*border_size)\n",
    "    ax.axis('off')\n",
    "\n",
    "    for idx, img in enumerate(images):\n",
    "        if idx >= grid_size[0] * grid_size[1]:\n",
    "            print(\"break\")\n",
    "            break\n",
    "\n",
    "        row = idx // grid_size[1]\n",
    "        col = idx % grid_size[1]\n",
    "        x = col * images.shape[2]\n",
    "        y = (grid_size[0] - 1 - row) * images.shape[1]\n",
    "\n",
    "        filename = other_props[\"filename\"][idx]\n",
    "        if filename.find(\"4-10keV\") != -1:\n",
    "            ax.imshow(img, extent=(x, x + images.shape[2], y, y + images.shape[1]), cmap='inferno', aspect='auto')\n",
    "        else:\n",
    "            ax.imshow(img, extent=(x, x + images.shape[2], y, y + images.shape[1]), cmap='viridis', aspect='auto')\n",
    "\n",
    "        uid = filename[filename.find(\"uid_\")+4:].split(\"_\")[0]\n",
    "        cluster_color = colormap(cluster_ids[idx])\n",
    "\n",
    "        cluster_nb = str(cluster_ids[idx])\n",
    "        fontsize = 4\n",
    "\n",
    "        rect = patches.Rectangle((x+border_size/2, y+border_size/2 ), images.shape[2]-2*border_size/4, images.shape[1]-2*border_size/4 , linewidth=border_size*3/2, edgecolor=cluster_color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.text(x + border_size+2, y + images.shape[1] - 4.0*border_size, uid, color='white', fontsize=fontsize, ha='left', va='bottom')\n",
    "        ax.text(x + border_size+2, y + 1.5*border_size, cluster_nb, color='white', fontsize=fontsize, ha='left', va='bottom')\n",
    "\n",
    "\n",
    "    plt.savefig(f\"outputs/test_visualized_clusters_{mode}{extra_line}_{len(images)}.png\", dpi=350)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if inverse:\n",
    "    extra_line = \"_inverse\"\n",
    "else:\n",
    "    extra_line = \"\"\n",
    "    \n",
    "train_dataloader, test_dataloader = prepare_dataloaders(mode=mode, im_size=im_size, batch_size=16, inverse=inverse)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_shape = (3, 224, 224)\n",
    "\n",
    "if mode == \"standard\":\n",
    "    im_shape = (1, im_size, im_size)\n",
    "    \n",
    "property_names = ['thermal_component', 'class', \"data\", \"filename\"]\n",
    "train_features, train_properties = extract_features(train_dataloader, property_names)\n",
    "test_features, test_properties = extract_features(test_dataloader, property_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_properties['thermal_component_vectorized'] = replace_string_values(train_properties['thermal_component'])\n",
    "test_properties['thermal_component_vectorized'] = replace_string_values(test_properties['thermal_component'])\n",
    "print(train_properties[\"data\"].shape)\n",
    "if len(train_properties[\"data\"].shape) != 4:\n",
    "    train_properties[\"data\"] = train_properties[\"data\"].reshape(train_features.shape[0], 3, *im_shape[1:])\n",
    "    test_properties[\"data\"] = test_properties[\"data\"].reshape(test_features.shape[0], 3, *im_shape[1:])\n",
    "print(train_properties[\"data\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curs = test_properties[\"data\"][test_properties[\"class\"]==4]\n",
    "for i in curs:\n",
    "    plt.imshow(i[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_features.reshape(-1,), density=True, bins=100)\n",
    "plt.hist(test_features.reshape(-1,), density=True, alpha=0.5, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your steps\n",
    "for n_clusters in [ 30, ]:\n",
    "    initial_steps = [\n",
    "        #{'type': 'pca', 'params': {'n_components': 0.9}},\n",
    "        {'type': 'tsne', 'params': {'n_components': 2, 'learning_rate': 'auto', 'init': 'random', 'perplexity': 300}},\n",
    "        {'type': 'kmeans', 'params': {'n_clusters': n_clusters}},\n",
    "        # Add other s\n",
    "    ]\n",
    "        \n",
    "    models_info, train_transformed = train_and_store_models(train_features, initial_steps)\n",
    "    models_info, test_transformed = apply_models_to_test_data(test_features, models_info)\n",
    "    \n",
    "    train_tsne, test_tsne = models_info[\"tsne_transformed_train_features\"], models_info[\"tsne_transformed_test_features\"]\n",
    "    print(models_info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "input_images = train_properties[\"data\"][:,:1] \n",
    "# Create an array of original indices\n",
    "original_indices = np.arange(len(input_images))\n",
    "\n",
    "# Resize images to 10x10\n",
    "resized_images = np.array([resize(image[0], (40, 40)) for image in input_images])\n",
    "\n",
    "cluster_labels = models_info[f\"{initial_steps[-1]['type']}_labels\"]\n",
    "# Sort images by cluster labels\n",
    "sorted_indices = cluster_labels.argsort()\n",
    "sorted_images = resized_images[sorted_indices]\n",
    "sorted_labels = cluster_labels[sorted_indices]\n",
    "sorted_original_indices = original_indices[sorted_indices]\n",
    "\n",
    "# Number of clusters\n",
    "cluster_labels_unqiue = np.unique(sorted_labels)\n",
    "num_clusters = cluster_labels_unqiue.size\n",
    "\n",
    "# Calculate median for each cluster\n",
    "cluster_medians = {i : np.mean(sorted_images[sorted_labels == i]) for i in np.unique(sorted_labels)}\n",
    "\n",
    "# Order clusters\n",
    "ordered_cluster_indices = order_clusters_by_median(cluster_medians, cluster_labels_unqiue)\n",
    "\n",
    "reordered_images, reordered_labels = reorder_images_by_cluster_order(sorted_images, sorted_labels, ordered_cluster_indices)\n",
    "reordered_original_indices, _ = reorder_images_by_cluster_order(sorted_original_indices, sorted_labels, ordered_cluster_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply the final order to all properties in train_properties\n",
    "reordered_train_properties = reorder_properties(train_properties, reordered_original_indices)\n",
    "\n",
    "indices_reduce = []#[0,2,5,6,7,12,13,17,18,20,25,26,28,29, 3, 23, 14, 1, 9, 21, 10, 19, 8]\n",
    "\n",
    "# Apply the function to filter images while maintaining order\n",
    "filtered_train_properties, new_reordered_original_indices, new_sorted_labels = filter_images_by_cluster(\n",
    "    reordered_train_properties, reordered_original_indices.tolist(), reordered_labels.tolist(), indices_reduce, n=37\n",
    ")\n",
    "new_sorted_labels = reordered_labels[new_reordered_original_indices]\n",
    "new_reordered_images = reordered_images[new_reordered_original_indices]\n",
    "filtered_train_properties = reorder_properties(reordered_train_properties, new_reordered_original_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "visualize_grid_with_clusters(new_reordered_images, new_sorted_labels, figsize=(40,40), grid_size=(70, 70), border_size=2, other_props=filtered_train_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_properties = train_properties\n",
    "cluster_labels=models_info[f\"{initial_steps[-1]['type']}_labels\"]\n",
    "names = {}\n",
    "for i in np.unique(cluster_labels):    \n",
    "    names[i] = current_properties[\"filename\"][cluster_labels==i]\n",
    "    \n",
    "with open(f'outputs/clusters_seed_{random_seed}{extra_line}_{initial_steps[-1][\"params\"][\"n_clusters\"]}.txt', 'w') as file:\n",
    "    for key, file_paths in names.items():\n",
    "        file.write(f\"{key}\\n\")\n",
    "        for file_path in file_paths:\n",
    "            file.write(f\"{file_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.unique(new_sorted_labels):\n",
    "    print(i)\n",
    "    print(np.sum(cluster_labels==i))\n",
    "    \n",
    "    visualize_np_images_from_cluster(i, new_sorted_labels, filtered_train_properties[\"data\"], max_images=500, norm_func=lambda x : x)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This creates a new file. If you want to use it, rename it as saved_filtered_filenames_final.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_filenames(filenames, filepath):\n",
    "    with open(filepath, 'w') as file:\n",
    "        for filename in filenames:\n",
    "            file.write(filename + '\\n')\n",
    "            \n",
    "cluster_to_keep = [1,3]\n",
    "indexes = np.array(range(len(new_sorted_labels)))\n",
    "filenames = []\n",
    "for cluster_label in cluster_to_keep:\n",
    "    filenames.extend(filtered_train_properties[\"filename\"][new_sorted_labels==cluster_label].tolist())\n",
    "    \n",
    "\n",
    "\n",
    "# Assuming updated_train_properties[\"filename\"] contains your list of filenames\n",
    "save_filepath = 'saved_filtered_filenames_new.txt'\n",
    "\n",
    "# Save the filenames to a text file\n",
    "save_filenames(filenames, save_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
