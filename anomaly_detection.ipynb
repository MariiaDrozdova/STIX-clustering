{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets import SolarDataset, normalize_standard, preprocess_clip_wrapper, preprocess_dino,load_file_names_and_classes_for_test, prepare_dataloaders, find_all_fits_files, load_filenames\n",
    "\n",
    "from utils import test_visualize_images_all_cluster_zero, visualize_batch_images_from_cluster, visualize_histograms_from_cluster, visualize_np_images_from_cluster\n",
    "\n",
    "im_size = 32\n",
    "mode=\"clip\"  \n",
    "from general import DATA_PATH, TEST_PATH\n",
    "\n",
    "import os\n",
    "import glob\n",
    "# Set the environment variable for the current session\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_shape = (3, 224, 224)\n",
    "\n",
    "if mode == \"standard\":\n",
    "    im_shape = (1, im_size, im_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = prepare_dataloaders(mode=mode, im_size=im_size, batch_size=16)\n",
    "\n",
    "property_names = ['thermal_component', 'class', \"data\", \"filename\"]\n",
    "train_features, train_properties = extract_features(train_dataloader, property_names)\n",
    "test_features, test_properties = extract_features(test_dataloader, property_names)\n",
    "\n",
    "train_properties['thermal_component_vectorized'] = replace_string_values(train_properties['thermal_component'])\n",
    "test_properties['thermal_component_vectorized'] = replace_string_values(test_properties['thermal_component'])\n",
    "\n",
    "if len(train_properties[\"data\"].shape) != 4:\n",
    "    train_properties[\"data\"] = train_properties[\"data\"].reshape(train_features.shape[0], 3, *im_shape[1:])\n",
    "    test_properties[\"data\"] = test_properties[\"data\"].reshape(test_features.shape[0], 3, *im_shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We first fit isolation forest to the test set, and then apply it to same test set to check what is considered as anomaly and then to big unlabaled set. This solution is higly dependent on prechosen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_features = test_features\n",
    "current_properties = test_properties\n",
    "\n",
    "isolation_forest = IsolationForest(n_estimators=900, contamination='auto', random_state=42)\n",
    "\n",
    "isolation_forest.fit(current_features)\n",
    "\n",
    "distances = isolation_forest.decision_function(current_features)\n",
    "cluster_labels_test = (distances <0.0)*1\n",
    "\n",
    "print(combine_predictions(cluster_labels_test, test_properties[\"class\"],))\n",
    "\n",
    "for i in np.unique(cluster_labels_test):\n",
    "    visualize_np_images_from_cluster(i, cluster_labels_test, current_properties[\"data\"], max_images=300, norm_func=lambda x : x)\n",
    "    plt.show()\n",
    "\n",
    "cluster_labels = (isolation_forest.decision_function(train_features) <0.0)*1\n",
    "\n",
    "for i in np.unique(cluster_labels):\n",
    "    visualize_np_images_from_cluster(i, cluster_labels, train_properties[\"data\"], max_images=900, norm_func=lambda x : x)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_np_images_from_cluster(cluster_num, cluster_assignments, images, max_images=32, norm_func=np.log1p):\n",
    "    cluster_indices = [i for i, cluster_id in enumerate(cluster_assignments) if cluster_id == cluster_num]\n",
    "\n",
    "    num_images = min(len(cluster_indices), max_images)\n",
    "\n",
    "    num_cols = int(math.ceil(math.sqrt(num_images)))\n",
    "    while num_images > num_cols * (num_cols // 4): \n",
    "        num_cols += 1\n",
    "\n",
    "    num_rows = max(1, num_cols // 4)\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(2 * num_cols, 2 * num_rows), squeeze=False)\n",
    "    axs = axs.flatten() \n",
    "\n",
    "    for ax in axs[num_images:]:  \n",
    "        ax.axis('off')\n",
    "\n",
    "    count = 0\n",
    "    for i in cluster_indices[:num_images]: \n",
    "        img = norm_func(images[i,0]) \n",
    "\n",
    "        axs[count].imshow(img, cmap='inferno') \n",
    "        axs[count].axis('off')\n",
    "        count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "isolation_forest = IsolationForest(n_estimators=900, contamination='auto', random_state=42)\n",
    "isolation_forest.fit(current_features)\n",
    "\n",
    "distances = isolation_forest.decision_function(current_features)\n",
    "cluster_labels_test = (distances < 0.0) * 1\n",
    "\n",
    "sorted_indices = np.argsort(distances)\n",
    "\n",
    "sorted_cluster_labels_test = cluster_labels_test[sorted_indices]\n",
    "sorted_test_data = current_properties[\"data\"][sorted_indices]\n",
    "\n",
    "for i in np.unique(sorted_cluster_labels_test):\n",
    "    cluster_indices = np.where(sorted_cluster_labels_test == i)[0]\n",
    "    \n",
    "    visualize_np_images_from_cluster(i, sorted_cluster_labels_test, sorted_test_data, max_images=300, norm_func=lambda x: x, )\n",
    "    plt.savefig(f\"outputs/isolation_forest_test_set_{i}.png\", dpi=300)\n",
    "\n",
    "train_distances = isolation_forest.decision_function(train_features)\n",
    "train_sorted_indices = np.argsort(train_distances)\n",
    "sorted_cluster_labels = (train_distances[train_sorted_indices] < 0.0) * 1\n",
    "sorted_train_data = train_properties[\"data\"][train_sorted_indices]\n",
    "\n",
    "for i in np.unique(sorted_cluster_labels):\n",
    "    cluster_indices = np.where(sorted_cluster_labels == i)[0]\n",
    "    \n",
    "    visualize_np_images_from_cluster(i, sorted_cluster_labels, sorted_train_data, max_images=900, norm_func=lambda x: x)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we train a small classifier on labeled test set and then apply to big unlabeled set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_for_training = test_properties[\"class\"]>1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CLIPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CLIPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)  \n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256) \n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 256)  \n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, 256)  \n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.fc5 = nn.Linear(256, num_classes)  \n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.bn3(self.fc3(x)))+x\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "input_dim = 512  \n",
    "num_classes = 2  \n",
    "\n",
    "model = CLIPClassifier(input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(input_dim,))\n",
    "max_ones = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "test_features_tensor = torch.tensor(test_features, dtype=torch.float)\n",
    "test_labels_tensor = torch.tensor(test_labels_for_training, dtype=torch.long)  # Use torch.long for labels\n",
    "\n",
    "clip_dataset = CLIPDataset(test_features_tensor, test_labels_tensor)\n",
    "train_loader = DataLoader(clip_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = train_loader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 300 \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    if epoch % 100 == 1:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "    \n",
    "    model.eval() \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    if epoch % 100 == 1:\n",
    "        print(f\"Validation Accuracy: {100 * correct / total}%\")\n",
    "        print(max_ones)\n",
    "\n",
    "    clip_test_dataset = CLIPDataset(train_features, train_properties[\"class\"])\n",
    "    model.eval()\n",
    "\n",
    "    train_test_loader = DataLoader(clip_test_dataset, batch_size=64, shuffle=False)\n",
    "    predicted_labels_classifier = []\n",
    "    for i, (features, labels) in enumerate(train_test_loader):\n",
    "        features = features \n",
    "        logits = model(features) \n",
    "        probabilities = F.softmax(logits, dim=1)  \n",
    "        predicted_labels = torch.argmax(probabilities, dim=1) \n",
    "        predicted_labels_classifier.extend([i for i in predicted_labels])\n",
    "    predicted_labels_classifier = np.array(predicted_labels_classifier).reshape(-1,)\n",
    "    model.train()\n",
    "    for i in np.unique(predicted_labels_classifier):\n",
    "        nb_ones = np.sum(predicted_labels_classifier==i)\n",
    "        if np.random.rand()>0.995:\n",
    "            max_ones = 0\n",
    "        if i==1:\n",
    "            print(nb_ones)\n",
    "        if i==1 and nb_ones > max_ones and correct / total > 0.99 and nb_ones > 100:\n",
    "            \n",
    "            max_ones = nb_ones\n",
    "            torch.save(model.state_dict(), f'weights/detector_at_samples_{max_ones}.pth')\n",
    "            if i==1:\n",
    "                visualize_np_images_from_cluster(i, predicted_labels_classifier, train_properties[\"data\"], max_images=300, norm_func=lambda x : x)\n",
    "                plt.show()\n",
    "\n",
    "print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As our test set is very small, we do have a validation set. For a fare study it could have been done manually taking a small subset of big unlabeled set. For the proof-of-concept, we suggest visually choose a checkpoint (defined by nb of anomalies in the big unlabeled set) whose anomalies look consistent. (hence so many visualization by the previous script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ones = 534\n",
    "model.load_state_dict(torch.load( f'weights/detector_at_samples_{max_ones}.pth'))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_test_dataset = CLIPDataset(train_features, train_properties[\"class\"])\n",
    "model.eval()\n",
    "\n",
    "train_test_loader = DataLoader(clip_test_dataset, batch_size=64, shuffle=False)\n",
    "predicted_labels_classifier = []\n",
    "for i, (features, labels) in enumerate(train_test_loader):\n",
    "    features = features \n",
    "    logits = model(features) \n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    predicted_labels = torch.argmax(probabilities, dim=1)  \n",
    "    predicted_labels_classifier.extend([i for i in predicted_labels])\n",
    " predicted_labels_classifier = np.array(predicted_labels_classifier).reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing clusters in the text file. Needed for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.unique(predicted_labels_classifier):\n",
    "    print(i)\n",
    "    print(np.sum(predicted_labels_classifier==i))\n",
    "    visualize_np_images_from_cluster(i, predicted_labels_classifier, train_properties[\"data\"], max_images=300, norm_func=lambda x : x)\n",
    "    plt.savefig(f\"outputs/isolation_forest/binary_classifier_class_{i}.png\")\n",
    "\n",
    "list_multiple_sources = train_properties[\"filename\"][predicted_labels_classifier==i]\n",
    "\n",
    "with open(f'outputs/multiple_sources_{max_ones}.txt', 'w') as file:\n",
    "    for item in list_multiple_sources:\n",
    "        file.write(item + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_clusters_file(clusters_file_path):\n",
    "    clusters = {}\n",
    "    current_cluster = None\n",
    "    with open(clusters_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.isdigit():  # This is a cluster number\n",
    "                current_cluster = int(line)\n",
    "                clusters[current_cluster] = []\n",
    "            elif line:  # This is a file path\n",
    "                if current_cluster is not None:\n",
    "                    clusters[current_cluster].append(line)\n",
    "    return clusters\n",
    "\n",
    "def parse_anomalies_file(anomalies_file_path):\n",
    "    anomalies = set()\n",
    "    with open(anomalies_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            anomalies.add(line.strip())\n",
    "    return anomalies\n",
    "\n",
    "def compute_anomalies_percentage(clusters, anomalies):\n",
    "    anomalies_percentage = {}\n",
    "    for cluster, files in clusters.items():\n",
    "        count = sum(1 for file in files if file in anomalies)\n",
    "        percentage = (count / len(files)) * 100 if files else 0\n",
    "        anomalies_percentage[cluster] = percentage\n",
    "    return anomalies_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 30\n",
    "clusters_file_path = f\"outputs/clusters_seed_45_{n_clusters}.txt\"\n",
    "anomalies_file_path = f\"outputs/multiple_sources_{max_ones}.txt\"\n",
    "\n",
    "clusters = parse_clusters_file(clusters_file_path)\n",
    "anomalies = parse_anomalies_file(anomalies_file_path)\n",
    "    \n",
    "anomalies_percentage = compute_anomalies_percentage(clusters, anomalies)\n",
    "    \n",
    "plt.hist(anomalies_percentage.values(), bins=15)\n",
    "plt.show()\n",
    "\n",
    "for cluster, percentage in anomalies_percentage.items():\n",
    "    print(f'Cluster {cluster}: {percentage:.2f}% anomalies')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "for i in anomalies_percentage.keys():\n",
    "    print(f'Cluster {i}: {anomalies_percentage[i]:.2f}% anomalies')\n",
    "    if anomalies_percentage[i] < 25:\n",
    "        continue\n",
    "    file_names = clusters[i]\n",
    "    nb_files = len(file_names)\n",
    "    grid_size = math.ceil(np.sqrt(nb_files))\n",
    "    images = []\n",
    "    for file_name in file_names:\n",
    "        with fits.open(file_name) as hdul:\n",
    "            image_data = hdul[0].data\n",
    "            image_data = (image_data - np.min(image_data))/(np.max(image_data) - np.min(image_data))\n",
    "            images.append(image_data)\n",
    "    images = np.array(images)\n",
    "    \n",
    "    random_seed = 45\n",
    "    def visualize_grid(images, grid_size=(90, 90), figsize=(18, 18)):\n",
    "        grid = np.zeros((grid_size[0] * images.shape[1], grid_size[1] * images.shape[2]))\n",
    "    \n",
    "        for idx, img in enumerate(images):\n",
    "            if idx >= grid_size[0] * grid_size[1]:\n",
    "                break  \n",
    "            row = idx // grid_size[1]\n",
    "            col = idx % grid_size[1]\n",
    "            grid[row * images.shape[1]:(row + 1) * images.shape[1],\n",
    "                 col * images.shape[2]:(col + 1) * images.shape[2]] = img\n",
    "    \n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.imshow(grid, cmap='inferno', aspect='auto')\n",
    "        plt.axis('off')  \n",
    "        plt.savefig(f\"weights/anomalies_{random_seed}_{anomalies_percentage[i]}.png\", dpi=350, )\n",
    "        plt.show()\n",
    "    \n",
    "    visualize_grid(images, figsize=(40,40), grid_size=(grid_size, grid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_percentage = {i: anomalies_percentage[i] for i in sorted(anomalies_percentage, key=anomalies_percentage.get, reverse=True)}\n",
    "for i in anomalies_percentage.keys():\n",
    "    print(f'Cluster {i}: {anomalies_percentage[i]:.2f}% anomalies')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "cluster_ids = list(anomalies_percentage.keys())\n",
    "colormap = cm.get_cmap('tab20b', max(cluster_ids) + 1)\n",
    "\n",
    "n = 30 \n",
    "top_n_clusters = list(anomalies_percentage.items())[:n]\n",
    "clusters, percentages = zip(*top_n_clusters) \n",
    "clusters = [f\"Cluster {i}\" for i, _ in top_n_clusters]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = [colormap(cluster_id) for cluster_id, _ in top_n_clusters]\n",
    "\n",
    "bars = plt.barh(clusters, percentages, color=colors)\n",
    "plt.gca().invert_yaxis() \n",
    "\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plt.tick_params(axis='y', which='both', left=False, right=False, labelleft=True)  # Keep y-axis labels\n",
    "\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    #plt.text(width,  # Position at the end of the bar\n",
    "    #         bar.get_y() + bar.get_height() / 2,\n",
    "    #         f'{width:.2f}%',  # The text to display\n",
    "    #         va='center', ha='right', color='black')  # Align text\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/anomaly_percentages.png\", dpi=350)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
